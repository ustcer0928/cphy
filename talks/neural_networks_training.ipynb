{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preamble / required packages\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "## Import local plotting functions and in-notebook display functions\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "## Comment this out to activate warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we train this?\n",
    "\n",
    "Gradient descent:\n",
    "\n",
    "$$\\theta_{i+1} = \\theta_i - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\theta_i}$$\n",
    "\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "### For a linear model:\n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n$$\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{2} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_j} = \\sum_{i=1}^n (\\hat{y}_i - y_i) x_{ij}$$\n",
    "\n",
    "$$\\theta_{i+1} = \\theta_i - \\eta \\sum_{i=1}^n (\\hat{y}_i - y_i) x_{ij}$$\n",
    "\n",
    "### For a logistic model:\n",
    "\n",
    "$$\\hat{y} = \\frac{1}{1 + e^{-\\theta_0 - \\theta_1 x_1 - \\theta_2 x_2 - \\cdots - \\theta_n x_n}}$$\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{i=1}^n y_i \\log \\hat{y}_i + (1 - y_i) \\log (1 - \\hat{y}_i)$$\n",
    "\n",
    "\n",
    "# How do we train this?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  23.,  231.,  734., 1300., 1351.,  843.,  362.,  110.,   40.,\n",
       "           9.]),\n",
       " array([ 1. ,  3.1,  5.2,  7.3,  9.4, 11.5, 13.6, 15.7, 17.8, 19.9, 22. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "population = np.arange(1000)\n",
    "\n",
    "all_overlaps = []\n",
    "for i in range(5003):\n",
    "    sample1 = np.random.choice(population, 100, replace=False)\n",
    "    sample2 = np.random.choice(population, 100, replace=False)\n",
    "\n",
    "    # number of overlaps\n",
    "    all_overlaps.append(len(set(sample1).intersection(sample2)))\n",
    "\n",
    "plt.hist(all_overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math is hard\n",
    "\n",
    "+ Computing symbolic gradients is exact but laborious\n",
    "+ Numerical gradients are approximate, easy, but expensive\n",
    "+ If the input to our network is $N$-dimensional, then we need to compute $N$ gradients. For a 1 Megapixel image, this is a million gradient operations per gradient descent step\n",
    "\n",
    "*Automatic differentiation* is a technique that can compute the *exact* gradient of a function in a computationally-efficient manner.\n",
    "\n",
    "*Backpropagation* is reverse-mode autodiff applied to deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.029225168711847025\n",
      "-0.4781751223381389\n"
     ]
    }
   ],
   "source": [
    "def double_well(x):\n",
    "    \"\"\"Double well potential function\"\"\"\n",
    "    return x**4 - 2*x**2\n",
    "\n",
    "def double_well_grad(x):\n",
    "    \"\"\"Derivative of double well potential function\"\"\"\n",
    "    return 4*x**3 - 4*x\n",
    "\n",
    "print(double_well(0.12132987))\n",
    "print(double_well_grad(0.12132987))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's write out these functions in terms of basic computational steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.029225168711847025\n",
      "-0.4781751223381389\n"
     ]
    }
   ],
   "source": [
    "def double_well_primitive(x):\n",
    "    \"\"\"Decompose the double well calculation into primitive operations\"\"\"\n",
    "    h1a = x**4\n",
    "    h1b = 2*x**2\n",
    "    h2 = h1a - h1b\n",
    "    return h2\n",
    "\n",
    "def double_well_primitive_grad(x):\n",
    "    \"\"\"Decompose the double well gradient calculation into primitive operations\"\"\"\n",
    "    dh2dh1a = 1\n",
    "    dh2dh1b = -1\n",
    "    dh1adx = 4*x**3\n",
    "    dh1bdx = 4*x\n",
    "    dh2dx = dh2dh1a * dh1adx + dh2dh1b * dh1bdx\n",
    "    return dh2dx\n",
    "\n",
    "print(double_well_primitive(0.12132987))\n",
    "print(double_well_primitive_grad(0.12132987))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A neural network example\n",
    "\n",
    "### Forward pass\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = \\sigma(\\boldsymbol{\\theta}_2 \\sigma(\\boldsymbol{\\theta}_1 \\mathbf{x}))\n",
    "$$\n",
    "for concreteness, we will use the sigmoid nonlinearity $\\sigma(x) = \\frac{1}{1 + e^{-x}}$.\n",
    "\n",
    "We start by breaking this into single steps\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_1^{u} = \\boldsymbol{\\theta}_1 \\mathbf{x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_1 = \\sigma(\\mathbf{h}_1^{u})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_2^{u} = \\boldsymbol{\\theta}_2 \\mathbf{h}_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = \\sigma(\\mathbf{h}_2^{u})\n",
    "$$\n",
    "\n",
    "\n",
    "### Backward pass\n",
    "\n",
    "Suppose we pass an input $\\mathbf{x}$ and get an output $\\hat{\\mathbf{y}}$, and we know the true output is $\\mathbf{y}$.\n",
    "\n",
    "If we are using the mean-squared error loss function, then\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{2} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2 = \\frac{1}{2} (\\hat{\\mathbf{y}} - \\mathbf{y})^\\top(\\hat{\\mathbf{y}} - \\mathbf{y})\n",
    "$$\n",
    "\n",
    "We want to update all the parameters $\\boldsymbol{\\theta}_1$ and $\\boldsymbol{\\theta}_2$ to minimize this loss.\n",
    "\n",
    "We traverse the network *backwards* in order to compute gradients.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\theta}_2} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\mathbf{y}}} \\frac{\\partial \\hat{\\mathbf{y}}}{\\partial \\mathbf{h}_2^{u}} \\frac{\\partial \\mathbf{h}_2^{u}}{\\partial \\boldsymbol{\\theta}_2}\n",
    "$$\n",
    "\n",
    "Solving this sequence from left-to-right,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\mathbf{y}}} = \\hat{\\mathbf{y}} - \\mathbf{y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{\\mathbf{y}}}{\\partial \\mathbf{h}_2^{u}} = \\hat{\\mathbf{y}} (1 - \\hat{\\mathbf{y}})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{h}_2^{u}}{\\partial \\boldsymbol{\\theta}_2} = \\mathbf{h}_1^\\top\n",
    "$$\n",
    "\n",
    "Putting this all together,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\theta}_2} = (\\hat{\\mathbf{y}} - \\mathbf{y}) \\odot \\hat{\\mathbf{y}} (1 - \\hat{\\mathbf{y}}) \\mathbf{h}_1^\\top\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Several things to notice: several of the \"intermediate\" values we computed during the forward pass reappear here. If we cached these values, then we don't have to recompute them during the backward pass. Also, notice that the sigmoid function's derivative is a function of its output. This is a common pattern in autodiff. In order to keep track of indicies and transposes, it helps to think about the dimensions of the input/output of each primitive operation. Additionally, it's useful to remember the some [rules for matrix multiplication:](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathbf{A} \\mathbf{B}}{\\partial \\mathbf{A}} = \\mathbf{B}^\\top, \\qquad\n",
    "\\frac{\\partial\\mathbf{A} \\mathbf{B}}{\\partial \\mathbf{B}} = \\mathbf{A}\n",
    "$$\n",
    "<!-- \n",
    "These left-right rules are also used in the derivation of the linear regression pseudoinverse global solution.\n",
    "\n",
    "Additionally, we should remember the chain rule for derivatives:\n",
    "$$\n",
    "\\frac{\\partial\\mathbf{A}(u) \\mathbf{B}(u)}{\\partial u} = \\frac{\\partial\\mathbf{A}(u)}{\\partial u} \\mathbf{B}(u) + \\mathbf{A}(u) \\frac{\\partial\\mathbf{B}(u)}{\\partial u}\n",
    "$$ -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random((2, 2))\n",
    "x = np.random.random((2, 1))\n",
    "\n",
    "def ff(x):\n",
    "    \"\"\"Feedforward pass\"\"\"\n",
    "    return np.dot(a, x)\n",
    "\n",
    "def ff_grad(x):\n",
    "    \"\"\"Gradient of feedforward pass\"\"\"\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harder cases: branching paths, loops, and shared weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let the computer do the work\n",
    "\n",
    "+ Most existing deep learning frameworks are built on top of autodiff\n",
    "+ Tensorflow, PyTorch, JAX, and many others allow you to specify the neural network as a function, and can compute the gradient automatically as long as everything is differentiable\n",
    "+ Implictly, these networks build a computation \"graph\" that is later traversed backwards to compute the gradient\n",
    "+ Caching forward pass values can speed up the backwards pass, since many derivatives depend on forward pass values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass value: -0.029225168711847025\n",
      "Analytic calculation backwards pass -0.4781751223381389\n",
      "Jax autodiff backwards pass -0.4781751\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "def double_well(x):\n",
    "    \"\"\"Double well potential function\"\"\"\n",
    "    return x**4 - 2*x**2\n",
    "\n",
    "print(\"Forward pass value:\", double_well_jax(0.12132987))\n",
    "print(\"Analytic calculation backwards pass\", double_well_grad(0.12132987))\n",
    "print(\"Jax autodiff backwards pass\", jax.grad(double_well)(0.12132987))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A multilayer perceptron classifier class\n",
    "\n",
    "class MLPClassifier:\n",
    "\n",
    "    def __init__(self, n_hidden, n_output, n_features, learning_rate=0.01, max_iter=1000, verbose=False):\n",
    "        \"\"\"Initialize the MLP classifier.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_hidden : int\n",
    "            Number of hidden units\n",
    "        n_output : int\n",
    "            Number of output units\n",
    "        n_features : int\n",
    "            Number of features\n",
    "        learning_rate : float, optional\n",
    "            Learning rate, by default 0.01\n",
    "        max_iter : int, optional\n",
    "            Maximum number of iterations, by default 1000\n",
    "        verbose : bool, optional\n",
    "            Print progress, by default False\n",
    "        \"\"\"\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.n_features = n_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights = self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights for the network.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary of weights\n",
    "        \"\"\"\n",
    "        weights = {\n",
    "            \"W1\": np.random.normal(0, 0.1, (self.n_features, self.n_hidden)),\n",
    "            \"b1\": np.zeros((1, self.n_hidden)),\n",
    "            \"W2\": np.random.normal(0, 0.1, (self.n_hidden, self.n_output)),\n",
    "            \"b2\": np.zeros((1, self.n_output))\n",
    "        }\n",
    "        return weights\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def _sigmoid_derivative(self, x):\n",
    "        \"\"\"Derivative of sigmoid activation function\"\"\"\n",
    "        return self._sigmoid(x) * (1 - self._sigmoid(x))\n",
    "\n",
    "    def _forward_pass(self, X):\n",
    "        \"\"\"Forward pass through the network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Input data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary of activations\n",
    "        \"\"\"\n",
    "        # Input layer\n",
    "        A1 = X\n",
    "        # Hidden layer\n",
    "        Z2 = A1 @ self.weights[\"W1\"] + self.weights[\"b1\"]\n",
    "        A2 = self._sigmoid(Z2)\n",
    "        # Output layer\n",
    "        Z3 = A2 @ self.weights[\"W2\"] + self.weights[\"b2\"]\n",
    "        A3 = self._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('cphy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e972983abb2b5c6293c34082f6ff1f6e60e8afbd2a068e0026ccecbb212fdb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
